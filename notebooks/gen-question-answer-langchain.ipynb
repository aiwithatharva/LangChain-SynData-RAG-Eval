{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain and Llama2: Question-Answer Generation with Output Parser\n",
    "\n",
    "This notebook uses LangChain and local Llama2-Chat inference that can be run on consumer grade hardware. \n",
    "\n",
    "\n",
    "The following LangChain features explored are:\n",
    "1) [LangChain Custom Prompt Template](https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/custom_prompt_template) for a Llama2-Chat model\n",
    "2) [Hugging Face Local Pipelines](https://python.langchain.com/docs/integrations/llms/huggingface_pipelines)\n",
    "3) [4-Bit Quantization](https://huggingface.co/blog/4bit-transformers-bitsandbytes)\n",
    "4) [Batch GPU Inference](https://python.langchain.com/docs/integrations/llms/huggingface_pipelines#batch-gpu-inference)\n",
    "5) [PydanticOutputParser](https://python.langchain.com/docs/modules/model_io/output_parsers/pydantic)\n",
    "6) [OutputFixingParser](https://python.langchain.com/docs/modules/model_io/output_parsers/output_fixing_parser)\n",
    "\n",
    "## Key Concepts: \n",
    "- This notebook is tailored towards using LangChain so it runs with local LLM models. At the time of writing several LangChain examples use the OpenAI as the LLM model but using Local LLMs pose considerable challenges that are addressed in this notebook. For example, local llms will have less parameters than OpenAI models and therefore produce lower-quality responses (e.g., following data structures).\n",
    "- [LangChain Output Parsers](https://python.langchain.com/docs/modules/model_io/output_parsers/) - many applications with LLMs will ultimately request structured data responses (e.g., json, markdown, etc.) and output parsers are used to return the required format. This notebook shows how to get required structured responses and then running these chains in batch mode. This is helpful because error messages are handled in a custom manner.\n",
    "- [Pydantic is all you need: Jason Liu](https://www.youtube.com/watch?v=yj-wSRJwrrc) - a Youtube video describing the benefits of using Pydantic parsers and data validation models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Inputs and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example context\n",
    "EXAMPLE_CONTEXT = \"\"\" \n",
    "The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France. \n",
    "It is named after the engineer Gustave Eiffel, whose company designed and built the tower from 1887 to 1889.\n",
    "The Eiffel Tower is 1,083 ft tall.\n",
    "\"\"\".strip()\n",
    "\n",
    "# Llama2-Chat Prompt Template\n",
    "llama2_template = \"\"\"\n",
    "<s>[INST] <<SYS>>\n",
    "{your_system_message}\n",
    "<</SYS>>\n",
    "\n",
    "{user_message_1} [/INST]\n",
    "\"\"\".strip()\n",
    "\n",
    "# System Message\n",
    "sys_template = \"\"\"\n",
    "You are a question and answer generating assistant. \n",
    "Given context, please generate a question and answer based on the information provided.\n",
    "Use only information from the context to answer the question.\n",
    "The question and answer should both be single sentences and no longer than 15 words.\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\".strip()\n",
    "\n",
    "# Human Message\n",
    "human_template = \"\"\"\n",
    "Context information is below. \n",
    "---------------------\n",
    "{context}\n",
    "---------------------\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\".strip()\n",
    "\n",
    "# Output Parser template to format instructions\n",
    "parser_template = \"\"\"\n",
    "It is mandatory the output be a markdown code snippet formatted in the following schema:\n",
    "\n",
    "```json\n",
    "{{\n",
    "\t\"QUESTION\": string  // Your question generated using only information from the context?,\n",
    "\t\"ANSWER\": string  // Your answer to the generated question.\n",
    "}}\n",
    "```\n",
    "\"\"\".strip()\n",
    "\n",
    "# Path to Model\n",
    "model_id = '/nvme4tb/Projects/llama2_models/Llama-2-7b-chat-hf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and packages\n",
    "import os, sys\n",
    "import gc\n",
    "import torch\n",
    "from time import time\n",
    "from torch import cuda, bfloat16\n",
    "from transformers import (AutoConfig,\n",
    "                          AutoTokenizer,\n",
    "                          AutoModelForCausalLM,\n",
    "                          BitsAndBytesConfig,\n",
    "                          pipeline)\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Custom Modules\n",
    "sys.path.append(os.getenv('SRC_DIRECTORY'))\n",
    "from src.lc_output_parsers import (insert_templates,\n",
    "                                   QAOutputFixingParser,\n",
    "                                   QuestionAnswerOutputParser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Output Parser - Inherent Pydantic Base Model\n",
    "\n",
    "The [QuestionAnswerOutputParser](../src/lc_output_parsers.py) is a custom output parser that demonstrates how to modify the existing [PydanticOutputParser](https://python.langchain.com/docs/modules/model_io/output_parsers/pydantic) to better handle parsing of Llama2-Chat responses for question and answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is mandatory the output be a markdown code snippet formatted in the following schema:\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"QUESTION\": string  // Your question generated using only information from the context?,\n",
      "\t\"ANSWER\": string  // Your answer to the generated question.\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Custom question-answer parser\n",
    "parser = QuestionAnswerOutputParser(parser_template=parser_template)\n",
    "\n",
    "# View the data structure formatting instructions \n",
    "print(parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Question-Answer Data Generation Prompt Template\n",
    "\n",
    "This section demonstrates how to create a [LangChain custom prompt template](https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/custom_prompt_template) for Llama2. The custom class could easily be modified to work with any LLM model of choice. Further, the input parameters (e.g., model_template, system_message, human_message) could be pointed to a prompt template databases for robust usage. \n",
    "\n",
    "**References**\n",
    "- [LangChain - Custom Agent With Tool Retrieval](https://python.langchain.com/docs/modules/agents/how_to/custom_agent_with_tool_retrieval)\n",
    "- [LangChain - Custom Prompt Template](https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/custom_prompt_template)\n",
    "- [LangChain - StringPromptTemplate](https://api.python.langchain.com/en/latest/prompts/langchain.prompts.base.StringPromptTemplate.html#langchain.prompts.base.StringPromptTemplate)\n",
    "- [Blog on LangChain and Llama2](https://www.mlexpert.io/prompt-engineering/langchain-quickstart-with-llama-2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] <<SYS>>\n",
      "You are a question and answer generating assistant. \n",
      "Given context, please generate a question and answer based on the information provided.\n",
      "Use only information from the context to answer the question.\n",
      "The question and answer should both be single sentences and no longer than 15 words.\n",
      "\n",
      "It is mandatory the output be a markdown code snippet formatted in the following schema:\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"QUESTION\": string  // Your question generated using only information from the context?,\n",
      "\t\"ANSWER\": string  // Your answer to the generated question.\n",
      "}\n",
      "```\n",
      "<</SYS>>\n",
      "\n",
      "Context information is below. \n",
      "---------------------\n",
      "The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France. \n",
      "It is named after the engineer Gustave Eiffel, whose company designed and built the tower from 1887 to 1889.\n",
      "The Eiffel Tower is 1,083 ft tall.\n",
      "---------------------\n",
      "\n",
      "It is mandatory the output be a markdown code snippet formatted in the following schema:\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"QUESTION\": string  // Your question generated using only information from the context?,\n",
      "\t\"ANSWER\": string  // Your answer to the generated question.\n",
      "}\n",
      "``` [/INST]\n"
     ]
    }
   ],
   "source": [
    "# Create a Llama2 template using LangChain PromptTemplate\n",
    "llama_template = insert_templates(model_template=llama2_template,\n",
    "                                  your_system_message=sys_template,\n",
    "                                  user_message_1=human_template)\n",
    "\n",
    "\n",
    "# Initialize Prompt Template\n",
    "llama2_prompt_template = PromptTemplate(\n",
    "    template=llama_template,\n",
    "    input_variables=['context'],\n",
    "    partial_variables={\n",
    "        \"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "del llama_template\n",
    "\n",
    "# Create the prompt using example context\n",
    "prompt = llama2_prompt_template.format(context=EXAMPLE_CONTEXT)\n",
    "\n",
    "# Print the template that would be passed to the LLM\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above template satisfies the Llama2-Chat format. Again, the above class can be modified to suit any LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model \n",
    "\n",
    "The following will form a LangChain [Chain](https://python.langchain.com/docs/modules/chains/) using their [LangChain Expression Language (LCEL)](https://python.langchain.com/docs/expression_language/). First, a `text-generation` model will be configured using 4-bit quantization and then the the above prompt template will be chained to the model. \n",
    "\n",
    "This will provide a chat model that can be invoked and queries will be generated.\n",
    "\n",
    "**REFERENCES**\n",
    "- [LangChain HuggingFace Pipeline](https://python.langchain.com/docs/integrations/llms/huggingface_pipelines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93edaf95c03f4e3ea92f39dd5ad66574",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Select the device\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "# Set quantization configuration to load large model with less GPU memory\n",
    "# this requires the `bitsandbytes` library\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16\n",
    ")\n",
    "\n",
    "# Model\n",
    "model_config = AutoConfig.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    ")\n",
    "model.eval() # set to evaluation for inference only\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Transformers pipeline\n",
    "pipe = pipeline(\n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True,  # langchain expects the full text\n",
    "    task='text-generation',\n",
    "    # we pass model parameters here too\n",
    "    # stopping_criteria=stopping_criteria,  # without this model rambles during chat\n",
    "    temperature=0.1,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
    "    max_new_tokens=200,  # max number of tokens to generate in the output\n",
    "    repetition_penalty=1.1  # without this output begins repeating\n",
    ")\n",
    "\n",
    "# LangChain Hugging Face Pipeline\n",
    "hf = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Nov 25 15:49:42 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 520.61.05    Driver Version: 520.61.05    CUDA Version: 11.8     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:01:00.0 Off |                  N/A |\n",
      "| 32%   58C    P2   143W / 350W |   2529MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  On   | 00000000:02:00.0 Off |                  N/A |\n",
      "|  0%   47C    P2   140W / 350W |   3163MiB / 24576MiB |      1%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A    769801      C   ...GenQuery/.venv/bin/python     2526MiB |\n",
      "|    1   N/A  N/A    769801      C   ...GenQuery/.venv/bin/python     3160MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# View GPU vRAM\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sure! Here's a question and answer based on the provided context:\n",
      "\n",
      "{\n",
      "\"QUESTION\": \"What is the height of the Eiffel Tower?\",\n",
      "\"ANSWER\": \"The Eiffel Tower is 1,083 feet (330 meters) tall.\"\n",
      "}\n",
      "\n",
      "Output after Passing Through Custom Parser\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Llama2QuestionAnswer(QUESTION='What is the height of the Eiffel Tower?', ANSWER='The Eiffel Tower is 1,083 feet (330 meters) tall.')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a chain\n",
    "chain = llama2_prompt_template | hf\n",
    "\n",
    "# Invoke the chain and get a response from Llama2\n",
    "output = chain.invoke({\"context\": EXAMPLE_CONTEXT})\n",
    "print(output)\n",
    "\n",
    "# Parse the output\n",
    "print(f'\\nOutput after Passing Through Custom Parser')\n",
    "parser.parse(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Auto-fixing Output Parser\n",
    "\n",
    "This output parser wraps another output parser, and in the event that the first one fails it calls out to another LLM to fix any errors.\n",
    "\n",
    "But we can do other things besides throw errors. Specifically, we can pass the misformatted output, along with the formatted instructions, to the model and ask it to fix it.\n",
    "\n",
    "**NOTE**: The default LangChain `PydanticOutputParser` wrapped in the `OutputFixingParser` was tested (not shown here) and it was not capable of fixing the misformatted output from Llama2-Chat. Therefore, a custom [QAOutputFixingParser](../src/lc_output_parsers.py) is presented here that is more effective for this use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llama2-Chat prompt template to fix incorrect json formatting\n",
    "output_fixing_template = \"\"\"\n",
    "<s>[INST] <<SYS>>\n",
    "You are a formatting expert and the Context must be re-formatted to meet the below Instructions.\n",
    "\n",
    "{instructions}\n",
    "<</SYS>>\n",
    "\n",
    "Context is provided below. \n",
    "---------------------\n",
    "{completion}\n",
    "--------------------- [/INST]\n",
    "\n",
    "No additional responses or wordings is allowed. Only respond in the Instructions format.\n",
    "\"\"\".strip()\n",
    "\n",
    "# Custom Output Fixing Parser for Question Answering\n",
    "fixing_parser = QAOutputFixingParser.from_llm(\n",
    "    parser=parser,\n",
    "    prompt=PromptTemplate.from_template(output_fixing_template),\n",
    "    llm=hf,\n",
    "    max_retries=1,\n",
    ")\n",
    "# Turn on to view the retry chain instructions\n",
    "fixing_parser.retry_chain.verbose = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo GOOD Output\n",
    "\n",
    "The below output will not be required to be sent to the `QAOutputFixingParser` because its formatted correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Llama2QuestionAnswer(QUESTION='This is a question?', ANSWER='Sounds good.')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple example of correctly formatted output\n",
    "good_output = \"\"\"\n",
    "{\"QUESTION\": \"This is a question?\",\n",
    "\"ANSWER\": \"Sounds good.\"}\n",
    "\"\"\".strip()\n",
    "fixing_parser.parse(good_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo Incorrectly Formatted Output\n",
    "\n",
    "The below output is slightly misformatted and be sent to the `QAOutputFixingParser` to fix its formatting.\n",
    "\n",
    "The misformatted errors in the below the errors are: 1) missing comma, 2) quotes on key-values.\n",
    "\n",
    "*Notice* the new template used in the LLM that asks it to fix the json formatting issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m<s>[INST] <<SYS>>\n",
      "You are a formatting expert and the Context must be re-formatted to meet the below Instructions.\n",
      "\n",
      "It is mandatory the output be a markdown code snippet formatted in the following schema:\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"QUESTION\": string  // Your question generated using only information from the context?,\n",
      "\t\"ANSWER\": string  // Your answer to the generated question.\n",
      "}\n",
      "```\n",
      "<</SYS>>\n",
      "\n",
      "Context is provided below. \n",
      "---------------------\n",
      "{QUESTION: This is a question?\n",
      "ANSWER: Sounds good.}\n",
      "--------------------- [/INST]\n",
      "\n",
      "No additional responses or wordings is allowed. Only respond in the Instructions format.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "QUESTION='This is a question?' ANSWER='Sounds good.'\n"
     ]
    }
   ],
   "source": [
    "# Error in the below the errors are: 1) missing comma, 2) quotes on key-values.\n",
    "bad_output = \"\"\" \n",
    "{QUESTION: This is a question?\n",
    "ANSWER: Sounds good.}\n",
    "\"\"\".strip()\n",
    "\n",
    "# Pass into the fixing parser\n",
    "output = fixing_parser.parse(bad_output)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo WRONGLY Formatted Output\n",
    "\n",
    "The below output is completely unacceptable and cannot be fixed by `QAOutputFixingParser` due to it missing information. \n",
    "\n",
    "In this instance we want to return a fixed string (i.e., NULL) and this demonstrates error handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m<s>[INST] <<SYS>>\n",
      "You are a formatting expert and the Context must be re-formatted to meet the below Instructions.\n",
      "\n",
      "It is mandatory the output be a markdown code snippet formatted in the following schema:\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"QUESTION\": string  // Your question generated using only information from the context?,\n",
      "\t\"ANSWER\": string  // Your answer to the generated question.\n",
      "}\n",
      "```\n",
      "<</SYS>>\n",
      "\n",
      "Context is provided below. \n",
      "---------------------\n",
      "\n",
      "--------------------- [/INST]\n",
      "\n",
      "No additional responses or wordings is allowed. Only respond in the Instructions format.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "QUESTION='NULL?' ANSWER='NUll'\n"
     ]
    }
   ],
   "source": [
    "# Very bad response wtih no results\n",
    "bad_output = \"\"\" \"\"\".strip()\n",
    "\n",
    "# Pass into the fixing parser to get response when it CANNOT be fixed\n",
    "output = fixing_parser.parse(bad_output)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above a question is returned based on the `EXAMPLE_CONTEXT` provided to Llama2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Synthetic Context-Query-Answer Results using Batch GPU Inference\n",
    "\n",
    "The below demonstrates how to synthetic datasets for IR and RAG evaluation of custom documents / text.\n",
    "\n",
    "Batch GPU Inferences is used with all the previously demonstrated custom prompts and output parsers.\n",
    "\n",
    "The demonstration data is a subset of the [Stanford Question Answering Dataset squad_v2](https://huggingface.co/datasets/squad_v2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load SquadV2 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.shape: (10, 4)\n",
      "Columns: ['id', 'context', 'question', 'answers']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2141</th>\n",
       "      <td>56df5cdd96943c1400a5d438</td>\n",
       "      <td>The Oklahoma City Police Department, has a uni...</td>\n",
       "      <td>How many substations does Oklahoma city have?</td>\n",
       "      <td>{'text': ['5'], 'answer_start': [182]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18339</th>\n",
       "      <td>5733823bd058e614000b5c03</td>\n",
       "      <td>The U.S. Federal Reserve and central banks aro...</td>\n",
       "      <td>What have central banks around the world done ...</td>\n",
       "      <td>{'text': ['expand money supplies'], 'answer_st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>56d37ac659d6e414001464d5</td>\n",
       "      <td>The two finalists were Kris Allen and Adam Lam...</td>\n",
       "      <td>Who were the final two contestants on season e...</td>\n",
       "      <td>{'text': ['Kris Allen and Adam Lambert'], 'ans...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>56cddc2a62d2951400fa690a</td>\n",
       "      <td>Christoph Waltz was cast in the role of Franz ...</td>\n",
       "      <td>Who did Christoph Waltz portray in Spectre?</td>\n",
       "      <td>{'text': ['Franz Oberhauser'], 'answer_start':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12083</th>\n",
       "      <td>5727cb4a4b864d1900163d30</td>\n",
       "      <td>Detroit and the rest of southeastern Michigan ...</td>\n",
       "      <td>What body of water affects Detroit's climate?</td>\n",
       "      <td>{'text': ['Great Lakes'], 'answer_start': [119]}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             id  \\\n",
       "2141   56df5cdd96943c1400a5d438   \n",
       "18339  5733823bd058e614000b5c03   \n",
       "980    56d37ac659d6e414001464d5   \n",
       "326    56cddc2a62d2951400fa690a   \n",
       "12083  5727cb4a4b864d1900163d30   \n",
       "\n",
       "                                                 context  \\\n",
       "2141   The Oklahoma City Police Department, has a uni...   \n",
       "18339  The U.S. Federal Reserve and central banks aro...   \n",
       "980    The two finalists were Kris Allen and Adam Lam...   \n",
       "326    Christoph Waltz was cast in the role of Franz ...   \n",
       "12083  Detroit and the rest of southeastern Michigan ...   \n",
       "\n",
       "                                                question  \\\n",
       "2141       How many substations does Oklahoma city have?   \n",
       "18339  What have central banks around the world done ...   \n",
       "980    Who were the final two contestants on season e...   \n",
       "326          Who did Christoph Waltz portray in Spectre?   \n",
       "12083      What body of water affects Detroit's climate?   \n",
       "\n",
       "                                                 answers  \n",
       "2141              {'text': ['5'], 'answer_start': [182]}  \n",
       "18339  {'text': ['expand money supplies'], 'answer_st...  \n",
       "980    {'text': ['Kris Allen and Adam Lambert'], 'ans...  \n",
       "326    {'text': ['Franz Oberhauser'], 'answer_start':...  \n",
       "12083   {'text': ['Great Lakes'], 'answer_start': [119]}  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "from types import SimpleNamespace\n",
    "\n",
    "# Specify paths to data, prompt templates, llama model, etc.\n",
    "paths = {'base_dir': Path.cwd().parents[0],\n",
    "         'prompt_vanilla': 'notebooks/question-answering-prompts/vanilla.txt',\n",
    "         'prompt_gbq': 'notebooks/question-answering-prompts/gbq.txt',\n",
    "         'squad_data': 'data/squad_v2',\n",
    "         }\n",
    "\n",
    "# Convert from dictionary to SimpleNamespace\n",
    "paths = SimpleNamespace(**paths)\n",
    "\n",
    "# Load squad_v2 data locally from disk\n",
    "df = load_dataset(str(paths.base_dir / paths.squad_data),\n",
    "                  split='train').to_pandas()\n",
    "\n",
    "# Remove redundant context\n",
    "df = df.drop_duplicates(subset=['context', 'title']).reset_index(drop=True)\n",
    "\n",
    "# Number of context samples for experimentation\n",
    "NUM_SAMPLES = 10\n",
    "\n",
    "# Randomly select contexts\n",
    "df = df.sample(n=NUM_SAMPLES, random_state=42)[['id', 'context', 'question', 'answers']]\n",
    "\n",
    "# Print Info.\n",
    "print(f'df.shape: {df.shape}')\n",
    "print(f'Columns: {df.columns.tolist()}')\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Inference with Chaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Time to Generate 10 Queries: 0.8 mins.\n",
      "Avg. Amount of Seconds Per Sample: 4.9\n"
     ]
    }
   ],
   "source": [
    "# Place the SquadV2 context in a list of dictionaries\n",
    "contexts = []\n",
    "for context in df.context.tolist():\n",
    "    contexts.append({'context': context})\n",
    "\n",
    "# Update the hf pipeline batch size\n",
    "hf.batch_size = 50\n",
    "\n",
    "# Turn off verbose in the retry chain instructions\n",
    "fixing_parser.retry_chain.verbose = False\n",
    "\n",
    "# Create a Chain\n",
    "chain = llama2_prompt_template | hf | fixing_parser\n",
    "\n",
    "# Start time\n",
    "st = time()\n",
    "\n",
    "# GPU Batch Inference\n",
    "results = chain.batch(contexts)\n",
    "\n",
    "# Total time to generate the queries\n",
    "total_secs = time() - st\n",
    "secs_per_sample = (total_secs / NUM_SAMPLES)\n",
    "print(f'Total Time to Generate {NUM_SAMPLES} Queries: {(total_secs / 60):.1f} mins.')\n",
    "print(f'Avg. Amount of Seconds Per Sample: {secs_per_sample:.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results and Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example # 1\n",
      "Context: The Oklahoma City Police Department, has a uniformed force of 1,169 officers and 300+ civilian employees. The Department has a central police station and five substations covering 2,500 police reporting districts that average 1/4 square mile in size.\n",
      "Original Query: How many substations does Oklahoma city have?\n",
      "Original Answer: 5\n",
      "Synthetic Query: How many police reporting districts does the Oklahoma City Police Department cover?\n",
      "Synthetic Answer: The Oklahoma City Police Department covers 2,500 police reporting districts that average 1/4 square mile in size.\n",
      "\n",
      "Example # 2\n",
      "Context: The U.S. Federal Reserve and central banks around the world have taken steps to expand money supplies to avoid the risk of a deflationary spiral, in which lower wages and higher unemployment lead to a self-reinforcing decline in global consumption. In addition, governments have enacted large fiscal stimulus packages, by borrowing and spending to offset the reduction in private sector demand caused by the crisis. The U.S. Federal Reserve's new and expanded liquidity facilities were intended to enable the central bank to fulfill its traditional lender-of-last-resort role during the crisis while mitigating stigma, broadening the set of institutions with access to liquidity, and increasing the flexibility with which institutions could tap such liquidity.\n",
      "Original Query: What have central banks around the world done to avoid the risk of a deflationary spiral?\n",
      "Original Answer: expand money supplies\n",
      "Synthetic Query: What steps has the U.S. Federal Reserve taken to mitigate the risk of a deflationary spiral?\n",
      "Synthetic Answer: The U.S. Federal Reserve has taken steps to expand money supplies and enact large fiscal stimulus packages to avoid a deflationary spiral.\n",
      "\n",
      "Example # 3\n",
      "Context: The two finalists were Kris Allen and Adam Lambert, both of whom had previously landed in the bottom three at the top five. Allen won the contest in the most controversial voting result since season two. It was claimed, later retracted, that 38 million of the 100 million votes cast on the night came from Allen's home state of Arkansas alone, and that AT&T employees unfairly influenced the votes by giving lessons on power-texting at viewing parties in Arkansas.\n",
      "Original Query: Who were the final two contestants on season eight of American Idol?\n",
      "Original Answer: Kris Allen and Adam Lambert\n",
      "Synthetic Query: Who were the two finalists in American Idol season 8?\n",
      "Synthetic Answer: Kris Allen and Adam Lambert.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Place the results into the data frame\n",
    "df['question_synthetic'] = [x.QUESTION for x in results]\n",
    "df['answer_synthetic'] = [x.ANSWER for x in results]\n",
    "\n",
    "# View a few examples\n",
    "for ii in range(3):\n",
    "    print(f'Example # {ii + 1}')\n",
    "    print(f'Context: {df.iloc[ii].context}')\n",
    "    print(f'Original Query: {df.iloc[ii].question}')\n",
    "    print(f'Original Answer: {df.iloc[ii].answers[\"text\"][0]}')\n",
    "    print(f'Synthetic Query: {df.iloc[ii].question_synthetic}')\n",
    "    print(f'Synthetic Answer: {df.iloc[ii].answer_synthetic}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Takeaway\n",
    "The above three examples show high-quality synthetic IR and RAG evaluation datasets for custom documents/text. Using this workflow is an attractive supplement to human annotation of custom documents.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
