{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Argument Retrieval Synthetic Query Generation\n",
    "\n",
    "This notebook illustrates the utilization of the Llama2-Chat prompt templates to create synthetic query-context data. It utilizes the [Llama-2-13b-chat-hf](https://huggingface.co/meta-llama), integrating seamlessly with the Hugging Face library for this purpose.\n",
    "\n",
    "The queries generated in this notebook are tailored towards argument retrieval tasks. A query in argument retrieval help users make decisions based on persuasive evidence or when they are engaged in debates, research, legal proceedings, or any argument where informed argumentation is crucial. Refer to the below references for more information on argument retrieval tasks and data:\n",
    "\n",
    "- **Touche 2020**:\n",
    "    - [Overview of Touch√© 2020: Argument Retrieval](https://ceur-ws.org/Vol-2696/paper_261.pdf)\n",
    "    - Hugging Face Dataset: [BeIR/webis-touche2020-generated-queries](https://huggingface.co/datasets/BeIR/webis-touche2020-generated-queries?row=0)\n",
    "\n",
    "- **Arguana**\n",
    "    - [Retrieval of the Best Counterargument without Prior Topic Knowledge](https://aclanthology.org/P18-1023.pdf)\n",
    "    - Hugging Face Dataset: [BeIR/arguana-generated-queries](https://huggingface.co/datasets/BeIR/arguana-generated-queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Inputs and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from types import SimpleNamespace\n",
    "\n",
    "# Specify paths to data, prompt templates, llama model, etc.\n",
    "paths = {'base_dir': Path.cwd().parents[0],\n",
    "         'prompt': 'notebooks/argument-retrieval-prompts/arg-prompt.txt',\n",
    "         'squad_data': 'data/squad_v2',\n",
    "         'model': '/nvme4tb/Projects/llama2_models/Llama-2-13b-chat-hf',\n",
    "         }\n",
    "\n",
    "# Number of context samples for experimentation\n",
    "NUM_SAMPLES = 3\n",
    "\n",
    "# Convert from dictionary to SimpleNamespace\n",
    "paths = SimpleNamespace(**paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and packages\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from IPython.display import clear_output\n",
    "import re\n",
    "import torch\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "The [Stanford Question Answering Dataset squad_v2](https://huggingface.co/datasets/squad_v2) dataset was downloaded from Hugging Face and stored locally. The Stanford Question Answering Dataset (SQuAD) is a collection of question-answer pairs derived from Wikipedia articles. In SQuAD, the correct answers of questions can be any sequence of tokens in the given text. Because the questions and answers are produced by humans through crowdsourcing, it is more diverse than some other question-answering datasets. \n",
    "\n",
    "If internet connection is available you can alternatively download the dataset as shown:\n",
    "```python\n",
    "df = load_dataset('squad_v2')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.shape: (19029, 5)\n",
      "Columns: ['id', 'title', 'context', 'question', 'answers']\n",
      "Number of Words in Context\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    19029.000000\n",
       "mean       116.600137\n",
       "std         49.666777\n",
       "min         20.000000\n",
       "25%         87.000000\n",
       "50%        107.000000\n",
       "75%        139.000000\n",
       "max        653.000000\n",
       "Name: num_words_context, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load squad_v2 data locally from disk\n",
    "df = load_dataset(str(paths.base_dir / paths.squad_data),\n",
    "                  split='train').to_pandas()\n",
    "\n",
    "# Remove redundant context\n",
    "df = df.drop_duplicates(subset=['context', 'title']).reset_index(drop=True)\n",
    "print(f'df.shape: {df.shape}')\n",
    "print(f'Columns: {df.columns.tolist()}')\n",
    "\n",
    "# Approximate the # of words in context\n",
    "df['num_words_context'] = df.context.apply(lambda x: len(x.split()))\n",
    "print('Number of Words in Context')\n",
    "display(df.num_words_context.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example # 1\n",
      "Context: The Oklahoma City Police Department, has a uniformed force of 1,169 officers and 300+ civilian employees. The Department has a central police station and five substations covering 2,500 police reporting districts that average 1/4 square mile in size.\n",
      "Squad Query: How many substations does Oklahoma city have?\n",
      "\n",
      "Example # 2\n",
      "Context: The U.S. Federal Reserve and central banks around the world have taken steps to expand money supplies to avoid the risk of a deflationary spiral, in which lower wages and higher unemployment lead to a self-reinforcing decline in global consumption. In addition, governments have enacted large fiscal stimulus packages, by borrowing and spending to offset the reduction in private sector demand caused by the crisis. The U.S. Federal Reserve's new and expanded liquidity facilities were intended to enable the central bank to fulfill its traditional lender-of-last-resort role during the crisis while mitigating stigma, broadening the set of institutions with access to liquidity, and increasing the flexibility with which institutions could tap such liquidity.\n",
      "Squad Query: What have central banks around the world done to avoid the risk of a deflationary spiral?\n",
      "\n",
      "Example # 3\n",
      "Context: The two finalists were Kris Allen and Adam Lambert, both of whom had previously landed in the bottom three at the top five. Allen won the contest in the most controversial voting result since season two. It was claimed, later retracted, that 38 million of the 100 million votes cast on the night came from Allen's home state of Arkansas alone, and that AT&T employees unfairly influenced the votes by giving lessons on power-texting at viewing parties in Arkansas.\n",
      "Squad Query: Who were the final two contestants on season eight of American Idol?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Randomly select 50 contexts\n",
    "df = df.sample(n=NUM_SAMPLES, random_state=42)[['id', 'context', 'question']]\n",
    "\n",
    "# View a few context and questions from original dataset\n",
    "for ii in range(NUM_SAMPLES):\n",
    "    print(f'Example # {ii + 1}')\n",
    "    print(f'Context: {df.iloc[ii].context}')\n",
    "    print(f'Squad Query: {df.iloc[ii].question}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Templates\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Template: arg-prompt.txt\n",
      "<s>[INST] <<SYS>>\n",
      "You are a question generating assistant for argument retrieval tasks. \n",
      "Given a document, please generate a simple and short question based on the information provided.\n",
      "The question needs to be focused seeking specific information or perspectives related to a document.\n",
      "the perspectives can be considered controversial.\n",
      "The question can be a maximum of 10 words long.\n",
      "Return only the question in the JSON format shown in the examples.\n",
      "\n",
      "For example, if a user is researching the impact of climate change on agriculture, their question might be:\n",
      "\n",
      "Example User question:\n",
      "\"What are the arguments for and against implementing sustainable agricultural practices to mitigate the effects of climate change?\"\n",
      "<</SYS>>\n",
      "\n",
      "\"DOCUMENT\": The Oklahoma City Police Department, has a uniformed force of 1,169 officers and 300+ civilian employees. The Department has a central police station and five substations covering 2,500 police reporting districts that average 1/4 square mile in size.\n",
      "{\"QUESTION\": Your question here.}\n",
      "[/INST]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the each prompt template and insert context\n",
    "for template_name in [paths.prompt]:\n",
    "    # Name of prompt template\n",
    "    print(f'Prompt Template: {(paths.base_dir / template_name).name}')\n",
    "    \n",
    "    # Load the prompt template \n",
    "    prompt_template = open(paths.base_dir / template_name, 'r').read()\n",
    "    \n",
    "    # Insert the context into the prompt template\n",
    "    prompts = [prompt_template.replace('[CONTEXT]', i) for i in df.context.tolist()]\n",
    "\n",
    "    # Example prompt for the first instance of data\n",
    "    print(f'{prompts[0]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Synthetic Queries\n",
    "\n",
    "This section will generate a synthetic query that would be similar to a user's query for argument retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ee0134fe5b743e9baae2247274ab046",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Oct 21 20:28:41 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 520.61.05    Driver Version: 520.61.05    CUDA Version: 11.8     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:01:00.0 Off |                  N/A |\n",
      "| 32%   45C    P2   108W / 350W |  14875MiB / 24576MiB |      7%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A    682423      C   ...GenQuery/.venv/bin/python    14872MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer and model\n",
    "tokenizer = LlamaTokenizer.from_pretrained(paths.model)\n",
    "model = LlamaForCausalLM.from_pretrained(paths.model,\n",
    "                                         load_in_8bit=True,\n",
    "                                         device_map='cuda:0',\n",
    "                                         torch_dtype=torch.float32)\n",
    "\n",
    "# View GPU vRAM\n",
    "!nvidia-smi\n",
    "\n",
    "# Notice: Llama-2 13B with 8bit quantization is ~14.8GB of vRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Time to Generate 3 Queries: 0.5 mins.\n",
      "Avg. Amount of Seconds Per Sample: 9.6\n"
     ]
    }
   ],
   "source": [
    "# Start time\n",
    "st = time()\n",
    "\n",
    "# Loop over each prompt template\n",
    "counter = 0\n",
    "prompt_names = []\n",
    "for template_name in [paths.prompt]:\n",
    "    # Name of prompt template\n",
    "    prompt_name = (paths.base_dir / template_name).name.split('.txt')[0]\n",
    "        \n",
    "    # Load the prompt template \n",
    "    prompt_template = open(paths.base_dir / template_name, 'r').read()\n",
    "    \n",
    "    # Insert the context into the prompt template\n",
    "    prompts = [prompt_template.replace('[CONTEXT]', i) for i in df.context.tolist()]\n",
    "    \n",
    "    # Loop over each prompt\n",
    "    llama_questions = []\n",
    "    for prompt in prompts:\n",
    "        # Tokenize the prompt\n",
    "        batch = tokenizer(prompt, return_tensors='pt')\n",
    "        \n",
    "        # Generate the response from Llama2\n",
    "        response = model.generate(batch[\"input_ids\"].cuda(),\n",
    "                                  do_sample=True,\n",
    "                                  top_k=50,\n",
    "                                  top_p=0.9,\n",
    "                                  temperature=0.6)\n",
    "        # Decode the response\n",
    "        decode_response = tokenizer.decode(response[0], skip_special_tokens=True)\n",
    "        llama_questions.append(decode_response)\n",
    "        clear_output()\n",
    "        counter += 1\n",
    "        \n",
    "    # Store llama queries in the dataframe\n",
    "    df[f'prompt_{prompt_name}'] = llama_questions\n",
    "    prompt_names.append(f'prompt_{prompt_name}')\n",
    "\n",
    "# Total time to generate the queries\n",
    "total_secs = time() - st\n",
    "secs_per_sample = (total_secs / counter)\n",
    "print(f'Total Time to Generate {counter} Queries: {(total_secs / 60):.1f} mins.')\n",
    "print(f'Avg. Amount of Seconds Per Sample: {secs_per_sample:.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the llama response to parse only the returned question\n",
    "def parse_response(text: str):\n",
    "    \n",
    "    # Extract Llama response\n",
    "    text = text.split('[/INST]')[-1].strip(\"</s>\").strip()\n",
    "\n",
    "    # Remove only the question\n",
    "    if 'question' in text.lower():\n",
    "        text = text.lower().split('question')[-1].split('?')[0].strip() + '?'\n",
    "    elif '?' in text:\n",
    "        text = text.split('?')[0].split('\\n')[-1] + '?'\n",
    "    else:\n",
    "        text = 'NAN'\n",
    "    text = re.sub('[\":]', '', text)\n",
    "    text = text.strip()\n",
    "    text = text.capitalize()\n",
    "    return text\n",
    "\n",
    "# Parse each llama response\n",
    "for prompt_name in prompt_names:\n",
    "    df[f'{prompt_name}_cleaned'] = df[f'{prompt_name}'].apply(lambda x: parse_response(text=x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example # 1\n",
      "Context: The Oklahoma City Police Department, has a uniformed force of 1,169 officers and 300+ civilian employees. The Department has a central police station and five substations covering 2,500 police reporting districts that average 1/4 square mile in size.\n",
      "Original Squad Query: How many substations does Oklahoma city have?\n",
      "Llama-2 Query: What are the arguments for and against increasing the size of the oklahoma city police department to address crime in the city's growing suburbs?\n",
      "\n",
      "Example # 2\n",
      "Context: The U.S. Federal Reserve and central banks around the world have taken steps to expand money supplies to avoid the risk of a deflationary spiral, in which lower wages and higher unemployment lead to a self-reinforcing decline in global consumption. In addition, governments have enacted large fiscal stimulus packages, by borrowing and spending to offset the reduction in private sector demand caused by the crisis. The U.S. Federal Reserve's new and expanded liquidity facilities were intended to enable the central bank to fulfill its traditional lender-of-last-resort role during the crisis while mitigating stigma, broadening the set of institutions with access to liquidity, and increasing the flexibility with which institutions could tap such liquidity.\n",
      "Original Squad Query: What have central banks around the world done to avoid the risk of a deflationary spiral?\n",
      "Llama-2 Query: What are the potential risks and benefits of the u.s. federal reserve's expanded liquidity facilities in mitigating the effects of the current economic crisis?\n",
      "\n",
      "Example # 3\n",
      "Context: The two finalists were Kris Allen and Adam Lambert, both of whom had previously landed in the bottom three at the top five. Allen won the contest in the most controversial voting result since season two. It was claimed, later retracted, that 38 million of the 100 million votes cast on the night came from Allen's home state of Arkansas alone, and that AT&T employees unfairly influenced the votes by giving lessons on power-texting at viewing parties in Arkansas.\n",
      "Original Squad Query: Who were the final two contestants on season eight of American Idol?\n",
      "Llama-2 Query: What were the allegations of voter manipulation and how did they impact the outcome of the competition?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View the examples\n",
    "for ii in range(NUM_SAMPLES):\n",
    "    print(f'Example # {ii + 1}')\n",
    "    print(f'Context: {df.iloc[ii].context}')\n",
    "    print(f'Original Squad Query: {df.iloc[ii][\"question\"]}')\n",
    "    print(f'Llama-2 Query: {df.iloc[ii][\"prompt_arg-prompt_cleaned\"]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Takeaways:\n",
    "- LLM prompting can generate queries tailored towards argument retrieval tasks.\n",
    "- Further experimentation with prompts can refine query outcomes as needed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
